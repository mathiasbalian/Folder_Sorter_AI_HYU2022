{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset\n",
    "\n",
    "We decide that our AI will only work on those subject:\n",
    "- Biology\n",
    "- Computer Science\n",
    "- Physics\n",
    "- Chemistry\n",
    "- Philosophy\n",
    "\n",
    "To make our AI understand which subject one file is in, we decide that if a file has some keys words, then it may be related to this subject.\n",
    "So, we have to create a dataset, where for each subject, there is a list of keys words. Our dataset is in the file 'Dataset_Topics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Dataset_Topics.txt\", \"r\")\n",
    "\n",
    "# We create a dictionary where the key is a school subject\n",
    "# and the value is a set of words related to this subject\n",
    "dataset = {\"biology\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"compsci\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"physics\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"chemistry\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"philosophy\": set(dict.fromkeys(f.readline().split(\";\")))}\n",
    "\n",
    "f.close()\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training/validation/testing set\n",
    "\n",
    "Now that we have our dataset, we need to create a training set, a validation set and a testing set. We have decided that our AI will just read PDF file only (possibly that in the future that we had other format). It will be easier to do a supervised learning. So, we'll just select a lot of file and labelised them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\apira\\anaconda3\\lib\\site-packages (2.11.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from PyPDF2) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [02:51<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "key = ['biology', 'compsci', 'physics', 'chemistry', 'philosophy']\n",
    "idx = dict()\n",
    "for i in range(0,len(key)):\n",
    "    idx[key[i]] = i\n",
    "\n",
    "# Path towards the folder where there are all files\n",
    "folder_path = os.path.abspath(os.getcwd()) + r'\\FileForTraining'\n",
    "\n",
    "# For each file, we will count\n",
    "scores = list()\n",
    "data_filename_topics = pd.read_csv('Dataset_fileName-Topics.csv')\n",
    "for filename, _ in tqdm(data_filename_topics.values):\n",
    "    file = os.path.join(folder_path, filename)\n",
    "    if(os.path.isfile(file)):\n",
    "        text = None\n",
    "        extension = os.path.splitext(file)[1]\n",
    "        if extension == \".pdf\":  # If the file is a pdf file\n",
    "            with open(file, 'rb') as pdfFileObj:\n",
    "                pdfReader = PyPDF2.PdfFileReader(pdfFileObj, strict = False)\n",
    "                text = re.sub(r'[^\\w\\s]', ' ', pdfReader.getPage(0).extractText())\n",
    "                for pageNumber in range(1, pdfReader.numPages):\n",
    "                    pageText = re.sub(r'[^\\w\\s]', ' ', pdfReader.getPage(pageNumber).extractText())\n",
    "                    text = ' '.join([text, pageText])\n",
    "\n",
    "                text = text.split(' ')\n",
    "\n",
    "        # If the file is a pdf, we can compute his score\n",
    "        if text != None:\n",
    "            score = np.zeros(len(key))\n",
    "            for word in text:\n",
    "                w = word.lower()\n",
    "                for subject in dataset:\n",
    "                    if(w in dataset[subject]):\n",
    "                        score[idx[subject]] += 1\n",
    "            scores.append(score)\n",
    "    else:\n",
    "        print(\"The file\", file, \"is not supported.\")\n",
    "\n",
    "# We decide to put all those information in dataframe\n",
    "df_x = pd.DataFrame(np.array(scores), columns = key)\n",
    "df_y = data_filename_topics['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     biology  compsci  physics  chemistry  philosophy\n",
      "0        0.0     21.0      0.0        4.0         1.0\n",
      "1        0.0     52.0      1.0        2.0         2.0\n",
      "2        1.0     98.0      1.0        3.0         6.0\n",
      "3        2.0    144.0      7.0        2.0        11.0\n",
      "4        0.0    143.0      3.0        4.0        10.0\n",
      "..       ...      ...      ...        ...         ...\n",
      "299    161.0     51.0     23.0       51.0        20.0\n",
      "300    615.0    256.0    209.0      115.0       150.0\n",
      "301     46.0    688.0   1763.0      671.0      1126.0\n",
      "302      3.0      7.0      2.0        2.0         7.0\n",
      "303      5.0    103.0    258.0       63.0       277.0\n",
      "\n",
      "[304 rows x 5 columns]\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "299    0\n",
      "300    0\n",
      "301    2\n",
      "302    4\n",
      "303    2\n",
      "Name: topic, Length: 304, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_x)\n",
    "print(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's analyse a bit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biology: 48\n",
      "compsci: 83\n",
      "physics: 96\n",
      "chemistry: 46\n",
      "philosophy: 31\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(key)):\n",
    "    print(f\"{key[i]}: {len(df_y[df_y == i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of files per topics isn't really well balanced but we'll work with that.\n",
    "\n",
    "Let's see, if we sort according to the greatest number of words in a topic, if this corresponds to the related topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct: 189 on 304 (62.17 %)\n"
     ]
    }
   ],
   "source": [
    "prediction_max = np.array([np.argmax(row) for row in df_x.values])\n",
    "print(f'Number of correct: {sum(df_y.values == prediction_max)} on {len(df_y)} ({sum(df_y.values == prediction_max)*100/len(df_y):.4} %)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that just selecting the topic with the most commun words doesn't always work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "Now that we have our dataframe, we have to split it into 3 sets : training, validation, testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.htmlNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch==1.12.0+cpu in c:\\users\\apira\\anaconda3\\lib\\site-packages (1.12.0+cpu)\n",
      "Requirement already satisfied: torchvision==0.13.0+cpu in c:\\users\\apira\\anaconda3\\lib\\site-packages (0.13.0+cpu)\n",
      "Requirement already satisfied: torchaudio==0.12.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torch==1.12.0+cpu) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (1.21.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.12.0+cpu torchvision==0.13.0+cpu torchaudio==0.12.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch as t\n",
    "\n",
    "# Split the data into 70% for training, 15% for validation and 15% for testing\n",
    "train_x, rest_x, train_y, rest_y = train_test_split(df_x.values, df_y.values, train_size=0.7, shuffle=True)\n",
    "val_x, test_x, val_y, test_y = train_test_split(rest_x, rest_y, train_size=0.5, shuffle=True)\n",
    "\n",
    "# Transformation and normalization\n",
    "train_x = t.tensor(train_x, dtype = t.float32)\n",
    "val_x = t.tensor(val_x, dtype = t.float32)\n",
    "test_x = t.tensor(test_x, dtype = t.float32)\n",
    "\n",
    "train_y = t.tensor(train_y, dtype= int)\n",
    "val_y = t.tensor(val_y, dtype= int)\n",
    "test_y = t.tensor(test_y, dtype= int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    # Inputs to hidden layer linear transformation\n",
    "    self.input = nn.Linear(D_in, H)\n",
    "    self.hidden = nn.Linear(H, H)\n",
    "    self.hidden2 = nn.Linear(H,H)\n",
    "    self.output = nn.Linear(H, D_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.input(x))\n",
    "    x = F.relu(self.hidden(x))\n",
    "    x = F.relu(self.hidden2(x))\n",
    "    y_pred = self.output(x)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_x, train_y, val_x, val_y, num_epochs = 10, batch_size = 64, show_info = False):\n",
    "  # Set model to train mode\n",
    "  model.train()\n",
    "\n",
    "  # Training loop\n",
    "  for epoch in range(0,num_epochs):\n",
    "    perm = t.randperm(len(train_y))\n",
    "    sum_loss = 0.\n",
    "\n",
    "    for i in range(0, len(train_y), batch_size):\n",
    "      x1 = Variable(train_x[perm[i:i + batch_size]], requires_grad=False)\n",
    "      y1 = Variable(train_y[perm[i:i + batch_size]], requires_grad=False)\n",
    "\n",
    "      # Reset gradient\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Forward\n",
    "      fx = model(x1)\n",
    "      loss = criterion(fx, y1)\n",
    "      \n",
    "      # Backward\n",
    "      loss.backward()\n",
    "      \n",
    "      # Update parameters\n",
    "      optimizer.step()\n",
    "      \n",
    "      sum_loss += loss.item()\n",
    "\n",
    "    val_loss = validation_model(model, criterion, val_x, val_y, batch_size)\n",
    "    if(show_info and epoch%10==0):\n",
    "      print(f\"Epoch: {epoch} \\tTraining Loss: {sum_loss} \\tValidation Loss: {val_loss}\")\n",
    "\n",
    "def validation_model(model, criterion, val_x, val_y, batch_size):\n",
    "  valid_loss = 0\n",
    "  perm = t.randperm(len(val_y))\n",
    "\n",
    "  # Set to validation mode\n",
    "  model.eval()\n",
    "  \n",
    "  for i in range(0, len(val_y), batch_size):\n",
    "      x1 = Variable(val_x[perm[i:i + batch_size]], requires_grad=False)\n",
    "      y1 = Variable(val_y[perm[i:i + batch_size]], requires_grad=False)\n",
    "      \n",
    "      # Forward\n",
    "      fx = model(x1)\n",
    "      loss = criterion(fx, y1)\n",
    "      \n",
    "      valid_loss += loss.item()\n",
    "\n",
    "  return valid_loss\n",
    "\n",
    "def evaluate_model(model, test_x, test_y):\n",
    "  model.eval()\n",
    "  y_pred = model(test_x)\n",
    "\n",
    "  y_pred = t.max(y_pred,1).indices\n",
    "  accuracy = t.sum(y_pred == test_y)/len(y_pred)\n",
    "  \n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 44.20754516124725 \tValidation Loss: 7.679714024066925\n",
      "Epoch: 10 \tTraining Loss: 25.072161182761192 \tValidation Loss: 18.944414913654327\n",
      "Epoch: 20 \tTraining Loss: 16.2971902936697 \tValidation Loss: 41.86445939540863\n",
      "Epoch: 30 \tTraining Loss: 11.428470591083169 \tValidation Loss: 51.15706565976143\n",
      "Epoch: 40 \tTraining Loss: 7.9906492829322815 \tValidation Loss: 36.69418954849243\n",
      "Epoch: 50 \tTraining Loss: 11.928312636911869 \tValidation Loss: 114.44620054960251\n",
      "Epoch: 60 \tTraining Loss: 8.908174134790897 \tValidation Loss: 172.9312653541565\n",
      "Epoch: 70 \tTraining Loss: 4.992218680679798 \tValidation Loss: 133.90561950206757\n",
      "Epoch: 80 \tTraining Loss: 6.442145840032026 \tValidation Loss: 131.05893683433533\n",
      "Epoch: 90 \tTraining Loss: 4.327533654868603 \tValidation Loss: 69.04219061136246\n",
      "Accuracy: 84.78260803222656 %\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "D_in, H, D_out = train_x.shape[1], 256, len(key)\n",
    "model = MLP(D_in, H, D_out)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_x, train_y,\n",
    "            val_x, val_y, epochs, batch_size, show_info = True)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = evaluate_model(model, test_x, test_y)*100\n",
    "print(f'Accuracy: {accuracy} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            biology  compsci  physics  chemistry  philosophy  Total\n",
      "biology           7        0        0          0           0      7\n",
      "compsci           0       13        1          0           0     14\n",
      "physics           1        0       12          1           0     14\n",
      "chemistry         1        0        0          4           1      6\n",
      "philosophy        1        1        0          0           3      5\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(test_x)\n",
    "y_pred = t.max(y_pred,1).indices\n",
    "\n",
    "key2 = key.copy()\n",
    "key2.append('Total')\n",
    "df_result = pd.DataFrame(np.zeros((len(key),len(key) + 1), dtype= int), columns = key2,  index = key)\n",
    "df_test_y = pd.DataFrame(test_y, dtype = int)\n",
    "df_y_pred = pd.DataFrame(y_pred, dtype = int)\n",
    "for i in range(0,len(key)):\n",
    "    l = df_test_y[df_y_pred[0] == i]\n",
    "    df_result.values[i][len(key)] = len(l)\n",
    "    for j in range(0,len(key)):\n",
    "        df_result.values[i][j] = len(l[l[0] == j])\n",
    "\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table, each row represents a kind of file that the neural network should initially associate. The columns represent the number of files that the neural network had associated to a subject.\n",
    "Thanks to this table, we can observe the file that our AI could mistake. We don't have many files for the testing set but we can observe that some computer science files could be mistaken with physics files.\n",
    "\n",
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = os.path.abspath(os.getcwd()) + '/save_model.pt'\n",
    "t.save(model, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How use our model?\n",
    "\n",
    "Firstly, you need to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (input): Linear(in_features=5, out_features=256, bias=True)\n",
       "  (hidden): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (hidden2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from tkinter import Tk, filedialog\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_model_path = os.path.abspath(os.getcwd()) + r'\\save_model.pt'\n",
    "load_model = t.load(load_model_path)\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, you have to select the folder where all the files, you want to sort, are stored. After that, you will find some folder with your files in, sorted by our AI, in the folder 'Files_sorted_by_AI'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan of the files in progress...\t(can take some times with several files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:31<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file C:/Users/apira/Downloads/Test\\biology is not supported.\n",
      "The file C:/Users/apira/Downloads/Test\\chemistry is not supported.\n",
      "The file C:/Users/apira/Downloads/Test\\compsci is not supported.\n",
      "The file C:/Users/apira/Downloads/Test\\philosophy is not supported.\n",
      "The file C:/Users/apira/Downloads/Test\\physics is not supported.\n",
      "Scan finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This the list of all topics that our AI knows and was train for\n",
    "key = ['biology', 'compsci', 'physics', 'chemistry', 'philosophy']\n",
    "\n",
    "root = Tk()\n",
    "root.withdraw()\n",
    "\n",
    "root.attributes('-topmost', True)\n",
    "folder_path = filedialog.askdirectory()\n",
    "\n",
    "# For each file, we will count\n",
    "scores = list()\n",
    "filename_list = list()\n",
    "print(\"Scan of the files in progress...\\t(can take some times with several files)\")\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    file = os.path.join(folder_path, filename)\n",
    "    if(os.path.isfile(file)):\n",
    "        text = None\n",
    "        extension = os.path.splitext(file)[1]\n",
    "        if extension == \".pdf\":  # If the file is a pdf file\n",
    "            with open(file, 'rb') as pdfFileObj:\n",
    "                pdfReader = PyPDF2.PdfFileReader(pdfFileObj, strict = False)\n",
    "                text = re.sub(r'[^\\w\\s]', ' ', pdfReader.getPage(0).extractText())\n",
    "                for pageNumber in range(1, pdfReader.numPages):\n",
    "                    pageText = re.sub(r'[^\\w\\s]', ' ', pdfReader.getPage(pageNumber).extractText())\n",
    "                    text = ' '.join([text, pageText])\n",
    "\n",
    "                text = text.split(' ')\n",
    "\n",
    "        # If the file is a pdf, we can compute his score\n",
    "        if text != None:\n",
    "            score = np.zeros(len(key))\n",
    "            for word in text:\n",
    "                w = word.lower()\n",
    "                for subject in dataset:\n",
    "                    if(w in dataset[subject]):\n",
    "                        score[idx[subject]] += 1\n",
    "            scores.append(score)\n",
    "            filename_list.append(file)\n",
    "    else:\n",
    "        print(\"The file\", file, \"is not supported.\")\n",
    "print(\"Scan finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where the AI will create the folder with the files sorted\n",
    "# The result will appear in the same folder that you have selected but if you want, you can modify the path\n",
    "files_sorted_path = folder_path\n",
    "\n",
    "data = t.tensor(np.array(scores), dtype = t.float32)\n",
    "\n",
    "# We predict to which topic are related each files\n",
    "load_model.eval()\n",
    "data_prediction = load_model(data)\n",
    "data_prediction = t.max(data_prediction,1).indices\n",
    "\n",
    "# We create the folder of the topics found by the AI\n",
    "for i in range(len(key)):\n",
    "    if i in data_prediction:\n",
    "        if not os.path.exists(files_sorted_path + f'/{key[i]}'):\n",
    "            os.makedirs(files_sorted_path + f'/{key[i]}')\n",
    "\n",
    "# We move all the files in the topic sorted by the AI\n",
    "for i in range(len(filename_list)):\n",
    "    shutil.move(filename_list[i], files_sorted_path + f\"/{key[data_prediction[i]]}/{os.path.basename(filename_list[i])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1862056fd60bcc08f314446936f371e203784f8dd6035e4c478d6b1676b9f55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
