{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset\n",
    "\n",
    "We decide that our AI will only work on those subject:\n",
    "- Biology\n",
    "- Computer Science\n",
    "- Physics\n",
    "- Chemistry\n",
    "- Philosophy\n",
    "\n",
    "To make our AI understand which subject one file is in, we decide that if a file has some keys words, then it may be related to this subject.\n",
    "So, we have to create a dataset, where for each subject, there is a list of keys words. Our dataset is in the file 'Dataset_Topics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Dataset_Topics.txt\", \"r\")\n",
    "\n",
    "# We create a dictionary where the key is a school subject\n",
    "# and the value is a set of words related to this subject\n",
    "dataset = {\"biology\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"compsci\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"physics\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"chemistry\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"philosophy\": set(dict.fromkeys(f.readline().split(\";\")))}\n",
    "\n",
    "f.close()\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training/validation/testing set\n",
    "\n",
    "Now that we have our dataset, we need to create a training set, a validation set and a testing set. We have decided that our AI will just read Word or PDF file only (possibly that in the future that we had other format). It will be easier to do a supervised learning. So, we'll just select a lot of file and labelised them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\apira\\anaconda3\\lib\\site-packages (2.11.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from PyPDF2) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:15<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "key = ['biology', 'compsci', 'physics', 'chemistry', 'philosophy']\n",
    "index = dict()\n",
    "for ind in range(0,len(key)):\n",
    "    index[key[ind]] = ind\n",
    "\n",
    "# Path towards the folder where there are all files\n",
    "folder_path = os.path.abspath(os.getcwd()) + '\\FileForTraining'\n",
    "\n",
    "# For each file, we will count\n",
    "scores = list()\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    total_counter = 0\n",
    "    file = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file):\n",
    "        text = None\n",
    "        extension = os.path.splitext(file)[1]\n",
    "        if extension == \".pdf\":  # If the file is a pdf file\n",
    "            with open(file, 'rb') as pdfFileObj:\n",
    "                pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "                text = re.sub(r'[^\\w\\s]', ' ', pdfReader.getPage(0).extractText())\n",
    "                for pageNumber in range(pdfReader.numPages):\n",
    "                    pageObj = pdfReader.getPage(pageNumber)\n",
    "                    pageText = re.sub(r'[^\\w\\s]', ' ', pageObj.extractText())\n",
    "                    text = ' '.join([text, pageText])\n",
    "\n",
    "                text = text.split(' ')\n",
    "        elif extension == \".doc\" or extension == \".docx\":  # If the file is a docx or doc\n",
    "            # text = textfromword(file)\n",
    "            pass\n",
    "\n",
    "        if text != None:\n",
    "            score = np.zeros(len(key))\n",
    "            for word in text:\n",
    "                w = word.lower()\n",
    "                for subject in dataset:\n",
    "                    if(w in dataset[subject]):\n",
    "                        score[index[subject]] += 1\n",
    "            scores.append(score)\n",
    "    else:\n",
    "        print(\"The file\", file, \"is not supported.\")\n",
    "\n",
    "# We decide to put all those information in dataframe\n",
    "dataframe = pd.DataFrame(np.array(scores), columns = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   biology  compsci  physics  chemistry  philosophy\n",
      "0      0.0     21.0      0.0        4.0         1.0\n",
      "1      0.0    153.0     15.0        1.0         1.0\n",
      "2      0.0    216.0      4.0        1.0        16.0\n",
      "3      0.0    297.0      8.0        0.0         6.0\n",
      "4      0.0    385.0      6.0        2.0         8.0\n",
      "    biology  compsci  physics  chemistry  philosophy\n",
      "8       0.0    143.0      3.0        4.0        10.0\n",
      "9       0.0    322.0      1.0       45.0         7.0\n",
      "10      0.0    336.0     16.0       22.0        23.0\n",
      "11      5.0    400.0     61.0        1.0        11.0\n",
      "12      0.0    242.0      2.0        4.0        19.0\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.head())\n",
    "print(dataframe.tail())\n",
    "\n",
    "# PathOfTheFileForTraining.txt is a file where is reported, to which study a file is related\n",
    "with open(os.path.abspath(os.getcwd()) + '\\PathOfTheFileForTraining.txt', 'r') as class_file:\n",
    "    y = [int(c.strip().split('\\t')[1]) for c in class_file.readlines()]\n",
    "\n",
    "print(y)\n",
    "# df = dataframe.copy()\n",
    "# df.insert(5, column='class', value=y)\n",
    "# df = df.drop(labels = 'class', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataframe, we have to split it into 3 sets : training, validation, testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.htmlNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch==1.12.0+cpu in c:\\users\\apira\\anaconda3\\lib\\site-packages (1.12.0+cpu)\n",
      "Requirement already satisfied: torchvision==0.13.0+cpu in c:\\users\\apira\\anaconda3\\lib\\site-packages (0.13.0+cpu)\n",
      "Requirement already satisfied: torchaudio==0.12.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torch==1.12.0+cpu) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (1.21.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.12.0+cpu torchvision==0.13.0+cpu torchaudio==0.12.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch as t\n",
    "\n",
    "# Split the data into 70% for training, 15% for validation and 15% for testing\n",
    "train_x, rest_x, train_y, rest_y = train_test_split(dataframe.values, y, train_size=0.7)\n",
    "val_x, test_x, val_y, test_y = train_test_split(rest_x, rest_y, train_size=0.5)\n",
    "\n",
    "# Transformation and normalization\n",
    "train_x = t.tensor(train_x, dtype = t.float32)\n",
    "val_x = t.tensor(val_x, dtype = t.float32)\n",
    "test_x = t.tensor(test_x, dtype = t.float32)\n",
    "\n",
    "train_y = t.tensor(train_y, dtype= int)\n",
    "val_y = t.tensor(val_y, dtype= int)\n",
    "test_y = t.tensor(test_y, dtype= int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    # Inputs to hidden layer linear transformation\n",
    "    self.input = nn.Linear(D_in, H)\n",
    "    self.hidden = nn.Linear(H, H)\n",
    "    self.output = nn.Linear(H, D_out)\n",
    "\n",
    "    self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.input(x))\n",
    "    x = F.relu(self.hidden(x))\n",
    "    y_pred = self.output(x)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_x, train_y, val_x, val_y, num_epochs = 10, batch_size = 64, show_info = False):\n",
    "  # Set model to train mode\n",
    "  model.train()\n",
    "\n",
    "  # Training loop\n",
    "  for epoch in range(0,num_epochs):\n",
    "    perm = t.randperm(len(train_y))\n",
    "    sum_loss = 0.\n",
    "\n",
    "    for i in range(0, len(train_y), batch_size):\n",
    "      x1 = Variable(train_x[perm[i:i + batch_size]], requires_grad=False)\n",
    "      y1 = Variable(train_y[perm[i:i + batch_size]], requires_grad=False)\n",
    "\n",
    "      # Reset gradient\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Forward\n",
    "      fx = model(x1)\n",
    "      loss = criterion(fx, y1)\n",
    "      \n",
    "      # Backward\n",
    "      loss.backward()\n",
    "      \n",
    "      # Update parameters\n",
    "      optimizer.step()\n",
    "      \n",
    "      sum_loss += loss.item()\n",
    "\n",
    "    val_loss = validation_model(model, criterion, val_x, val_y, batch_size)\n",
    "    if(show_info):\n",
    "      print(f\"Epoch: {epoch+1}\\tTraining Loss: {sum_loss}\\tValidation Loss: {val_loss}\")\n",
    "\n",
    "def validation_model(model, criterion, val_x, val_y, batch_size):\n",
    "  valid_loss = 0\n",
    "  perm = t.randperm(len(val_y))\n",
    "\n",
    "  # Set to validation mode\n",
    "  model.eval()\n",
    "  \n",
    "  for i in range(0, len(val_y), batch_size):\n",
    "      x1 = Variable(val_x[perm[i:i + batch_size]], requires_grad=False)\n",
    "      y1 = Variable(val_y[perm[i:i + batch_size]], requires_grad=False)\n",
    "      \n",
    "      # Forward\n",
    "      fx = model(x1)\n",
    "      loss = criterion(fx, y1)\n",
    "      \n",
    "      valid_loss += loss.item()\n",
    "\n",
    "  return valid_loss\n",
    "\n",
    "def evaluate_model(model, test_x, test_y):\n",
    "  model.eval()\n",
    "  y_pred = model(test_x)\n",
    "\n",
    "  y_pred = t.max(y_pred,1).indices\n",
    "  accuracy =  t.mean(t.Tensor([i == j for i, j in zip(y_pred, test_y)]))\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTraining Loss: 25.636178970336914\tValidation Loss: 0.0\n",
      "Epoch: 2\tTraining Loss: 0.0\tValidation Loss: 0.0\n",
      "Epoch: 3\tTraining Loss: 0.0\tValidation Loss: 0.0\n",
      "Epoch: 4\tTraining Loss: 0.0\tValidation Loss: 0.0\n",
      "Epoch: 5\tTraining Loss: 0.0\tValidation Loss: 0.0\n",
      "Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-2\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "D_in, H, D_out = train_x.shape[1], 256, len(key)\n",
    "model = MLP(D_in, H, D_out)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_x, train_y,\n",
    "            val_x, val_y, epochs, batch_size, show_info = True)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = evaluate_model(model, test_x, test_y)*100\n",
    "print(f'Accuracy: {accuracy} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1862056fd60bcc08f314446936f371e203784f8dd6035e4c478d6b1676b9f55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
