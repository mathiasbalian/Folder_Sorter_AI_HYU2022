{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset\n",
    "\n",
    "We decide that our AI will only work on those subject:\n",
    "- Biology\n",
    "- Computer Science\n",
    "- Physics\n",
    "- Chemistry\n",
    "- Philosophy\n",
    "\n",
    "To make our AI understand which subject one file is in, we decide that if a file has some keys words, then it may be related to this subject.\n",
    "So, we have to create a dataset, where for each subject, there is a list of keys words. Our dataset is in the file 'Dataset_Topics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Dataset_Topics.txt\", \"r\")\n",
    "\n",
    "# We create a dictionary where the key is a school subject\n",
    "# and the value is a set of words related to this subject\n",
    "dataset = {\"biology\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"compsci\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"physics\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"chemistry\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"philosophy\": set(dict.fromkeys(f.readline().split(\";\")))}\n",
    "\n",
    "f.close()\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training/validation/testing set\n",
    "\n",
    "Now that we have our dataset, we need to create a training set, a validation set and a testing set. We have decided that our AI will just read PDF file only (possibly that in the future that we had other format). It will be easier to do a supervised learning. So, we'll just select a lot of file and labelised them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\apira\\anaconda3\\lib\\site-packages (2.11.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from PyPDF2) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [02:57<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "key = ['biology', 'compsci', 'physics', 'chemistry', 'philosophy']\n",
    "idx = dict()\n",
    "for i in range(0,len(key)):\n",
    "    idx[key[i]] = i\n",
    "\n",
    "# Path towards the folder where there are all files\n",
    "folder_path = os.path.abspath(os.getcwd()) + '\\FileForTraining'\n",
    "\n",
    "# For each file, we will count\n",
    "scores = list()\n",
    "data_filename_topics = pd.read_csv('Dataset_fileName-Topics.csv')\n",
    "for filename, _ in tqdm(data_filename_topics.values):\n",
    "    file = os.path.join(folder_path, filename)\n",
    "    if(os.path.isfile(file)):\n",
    "        text = None\n",
    "        extension = os.path.splitext(file)[1]\n",
    "        if extension == \".pdf\":  # If the file is a pdf file\n",
    "            with open(file, 'rb') as pdfFileObj:\n",
    "                pdfReader = PyPDF2.PdfFileReader(pdfFileObj, strict = False)\n",
    "                text = re.sub(r'[^\\w\\s]', ' ', pdfReader.getPage(0).extractText())\n",
    "                for pageNumber in range(1, pdfReader.numPages):\n",
    "                    pageText = re.sub(r'[^\\w\\s]', ' ', pdfReader.getPage(pageNumber).extractText())\n",
    "                    text = ' '.join([text, pageText])\n",
    "\n",
    "                text = text.split(' ')\n",
    "\n",
    "        # If the file is a pdf, we can compute his score\n",
    "        if text != None:\n",
    "            score = np.zeros(len(key))\n",
    "            for word in text:\n",
    "                w = word.lower()\n",
    "                for subject in dataset:\n",
    "                    if(w in dataset[subject]):\n",
    "                        score[idx[subject]] += 1\n",
    "            scores.append(score)\n",
    "    else:\n",
    "        print(\"The file\", file, \"is not supported.\")\n",
    "\n",
    "# We decide to put all those information in dataframe\n",
    "df_x = pd.DataFrame(np.array(scores), columns = key)\n",
    "df_y = data_filename_topics['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     biology  compsci  physics  chemistry  philosophy\n",
      "0        0.0     21.0      0.0        4.0         1.0\n",
      "1        0.0     52.0      1.0        2.0         2.0\n",
      "2        1.0     98.0      1.0        3.0         6.0\n",
      "3        2.0    144.0      7.0        2.0        11.0\n",
      "4        0.0    143.0      3.0        4.0        10.0\n",
      "..       ...      ...      ...        ...         ...\n",
      "299    161.0     51.0     23.0       51.0        20.0\n",
      "300    615.0    256.0    209.0      115.0       150.0\n",
      "301     46.0    688.0   1763.0      671.0      1126.0\n",
      "302      3.0      7.0      2.0        2.0         7.0\n",
      "303      5.0    103.0    258.0       63.0       277.0\n",
      "\n",
      "[304 rows x 5 columns]\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "299    0\n",
      "300    0\n",
      "301    2\n",
      "302    4\n",
      "303    2\n",
      "Name: topic, Length: 304, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_x)\n",
    "print(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's analyse a bit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biology: 48\n",
      "compsci: 83\n",
      "physics: 96\n",
      "chemistry: 46\n",
      "philosophy: 31\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(key)):\n",
    "    print(f\"{key[i]}: {len(df_y[df_y == i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of files per topics isn't really well balanced but we'll work with that.\n",
    "\n",
    "Let's see, if we sort according to the greatest number of words in a topic, if this corresponds to the related topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct: 189 on 304 (62.17 %)\n"
     ]
    }
   ],
   "source": [
    "prediction_max = np.array([np.argmax(row) for row in df_x.values])\n",
    "print(f'Number of correct: {sum(df_y.values == prediction_max)} on {len(df_y)} ({sum(df_y.values == prediction_max)*100/len(df_y):.4} %)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that just selecting the topic with the most commun words doesn't always work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "Now that we have our dataframe, we have to split it into 3 sets : training, validation, testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.htmlNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch==1.12.0+cpu in c:\\users\\apira\\anaconda3\\lib\\site-packages (1.12.0+cpu)\n",
      "Requirement already satisfied: torchvision==0.13.0+cpu in c:\\users\\apira\\anaconda3\\lib\\site-packages (0.13.0+cpu)\n",
      "Requirement already satisfied: torchaudio==0.12.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torch==1.12.0+cpu) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (1.21.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.12.0+cpu torchvision==0.13.0+cpu torchaudio==0.12.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch as t\n",
    "\n",
    "# Split the data into 70% for training, 15% for validation and 15% for testing\n",
    "train_x, rest_x, train_y, rest_y = train_test_split(df_x.values, df_y.values, train_size=0.7, shuffle=True)\n",
    "val_x, test_x, val_y, test_y = train_test_split(rest_x, rest_y, train_size=0.5, shuffle=True)\n",
    "\n",
    "# Transformation and normalization\n",
    "train_x = t.tensor(train_x, dtype = t.float32)\n",
    "val_x = t.tensor(val_x, dtype = t.float32)\n",
    "test_x = t.tensor(test_x, dtype = t.float32)\n",
    "\n",
    "train_y = t.tensor(train_y, dtype= int)\n",
    "val_y = t.tensor(val_y, dtype= int)\n",
    "test_y = t.tensor(test_y, dtype= int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    # Inputs to hidden layer linear transformation\n",
    "    self.input = nn.Linear(D_in, H)\n",
    "    self.hidden = nn.Linear(H, H)\n",
    "    self.hidden2 = nn.Linear(H,H)\n",
    "    self.output = nn.Linear(H, D_out)\n",
    "\n",
    "    self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.input(x))\n",
    "    x = F.relu(self.hidden(x))\n",
    "    x = F.relu(self.hidden2(x))\n",
    "    y_pred = self.output(x)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_x, train_y, val_x, val_y, num_epochs = 10, batch_size = 64, show_info = False):\n",
    "  # Set model to train mode\n",
    "  model.train()\n",
    "\n",
    "  # Training loop\n",
    "  for epoch in range(0,num_epochs):\n",
    "    perm = t.randperm(len(train_y))\n",
    "    sum_loss = 0.\n",
    "\n",
    "    for i in range(0, len(train_y), batch_size):\n",
    "      x1 = Variable(train_x[perm[i:i + batch_size]], requires_grad=False)\n",
    "      y1 = Variable(train_y[perm[i:i + batch_size]], requires_grad=False)\n",
    "\n",
    "      # Reset gradient\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Forward\n",
    "      fx = model(x1)\n",
    "      loss = criterion(fx, y1)\n",
    "      \n",
    "      # Backward\n",
    "      loss.backward()\n",
    "      \n",
    "      # Update parameters\n",
    "      optimizer.step()\n",
    "      \n",
    "      sum_loss += loss.item()\n",
    "\n",
    "    val_loss = validation_model(model, criterion, val_x, val_y, batch_size)\n",
    "    if(show_info and epoch%10==0):\n",
    "      print(f\"Epoch: {epoch} \\tTraining Loss: {sum_loss} \\tValidation Loss: {val_loss}\")\n",
    "\n",
    "def validation_model(model, criterion, val_x, val_y, batch_size):\n",
    "  valid_loss = 0\n",
    "  perm = t.randperm(len(val_y))\n",
    "\n",
    "  # Set to validation mode\n",
    "  model.eval()\n",
    "  \n",
    "  for i in range(0, len(val_y), batch_size):\n",
    "      x1 = Variable(val_x[perm[i:i + batch_size]], requires_grad=False)\n",
    "      y1 = Variable(val_y[perm[i:i + batch_size]], requires_grad=False)\n",
    "      \n",
    "      # Forward\n",
    "      fx = model(x1)\n",
    "      loss = criterion(fx, y1)\n",
    "      \n",
    "      valid_loss += loss.item()\n",
    "\n",
    "  return valid_loss\n",
    "\n",
    "def evaluate_model(model, test_x, test_y):\n",
    "  model.eval()\n",
    "  y_pred = model(test_x)\n",
    "\n",
    "  y_pred = t.max(y_pred,1).indices\n",
    "  accuracy = t.sum(y_pred == test_y)/len(y_pred)\n",
    "  \n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 48.503595769405365 \tValidation Loss: 14.585851907730103\n",
      "Epoch: 10 \tTraining Loss: 31.11023785918951 \tValidation Loss: 8.923621118068695\n",
      "Epoch: 20 \tTraining Loss: 11.212598226964474 \tValidation Loss: 4.466354936361313\n",
      "Epoch: 30 \tTraining Loss: 9.06130476295948 \tValidation Loss: 5.072529688477516\n",
      "Epoch: 40 \tTraining Loss: 7.096194840967655 \tValidation Loss: 5.090612441301346\n",
      "Epoch: 50 \tTraining Loss: 22.226621463894844 \tValidation Loss: 7.405122563242912\n",
      "Epoch: 60 \tTraining Loss: 6.410949652083218 \tValidation Loss: 7.692902863025665\n",
      "Epoch: 70 \tTraining Loss: 11.587390199303627 \tValidation Loss: 5.829236976802349\n",
      "Epoch: 80 \tTraining Loss: 3.534953062655404 \tValidation Loss: 7.492872446775436\n",
      "Epoch: 90 \tTraining Loss: 3.3155347015708685 \tValidation Loss: 8.136256963014603\n",
      "Accuracy: 82.60869598388672 %\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "D_in, H, D_out = train_x.shape[1], 256, len(key)\n",
    "model = MLP(D_in, H, D_out)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_x, train_y,\n",
    "            val_x, val_y, epochs, batch_size, show_info = True)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = evaluate_model(model, test_x, test_y)*100\n",
    "print(f'Accuracy: {accuracy} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            biology  compsci  physics  chemistry  philosophy  Total\n",
      "biology           4        0        0          1           0      5\n",
      "compsci           0       12        3          0           0     15\n",
      "physics           0        0       13          1           1     15\n",
      "chemistry         0        0        1          4           0      5\n",
      "philosophy        1        0        0          0           5      6\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(test_x)\n",
    "y_pred = t.max(y_pred,1).indices\n",
    "\n",
    "key2 = key.copy()\n",
    "key2.append('Total')\n",
    "df_result = pd.DataFrame(np.zeros((len(key),len(key) + 1), dtype= int), columns = key2,  index = key)\n",
    "df_test_y = pd.DataFrame(test_y, dtype = int)\n",
    "df_y_pred = pd.DataFrame(y_pred, dtype = int)\n",
    "for i in range(0,len(key)):\n",
    "    l = df_test_y[df_y_pred[0] == i]\n",
    "    df_result.values[i][len(key)] = len(l)\n",
    "    for j in range(0,len(key)):\n",
    "        df_result.values[i][j] = len(l[l[0] == j])\n",
    "\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table, each row represents a kind of file that the neural network should initially associate. The columns represent the number of files that the neural network had associated to a subject.\n",
    "Thanks to this table, we can observe the file that our AI could mistake. We don't have many files for the testing set but we can observe that some computer science files could be mistaken with physics files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1862056fd60bcc08f314446936f371e203784f8dd6035e4c478d6b1676b9f55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
