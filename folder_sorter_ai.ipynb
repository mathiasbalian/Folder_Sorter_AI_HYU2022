{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset\n",
    "\n",
    "We decide that our AI will only work on those subject:\n",
    "- Biology\n",
    "- Computer Science\n",
    "- Physics\n",
    "- Chemistry\n",
    "- Philosophy\n",
    "\n",
    "To make our AI understand which subject one file is in, we decide that if a file has some keys words, then it may be related to this subject.\n",
    "So, we have to create a dataset, where for each subject, there is a list of keys words. Our dataset is in the file 'Dataset_Topics.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Dataset_Topics.txt\", \"r\")\n",
    "\n",
    "# We create a dictionary where the key is a school subject\n",
    "# and the value is a set of words related to this subject\n",
    "dataset = {\"biology\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"compsci\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"physics\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"chemistry\": set(dict.fromkeys(f.readline().split(\";\"))),\n",
    "           \"philosophy\": set(dict.fromkeys(f.readline().split(\";\")))}\n",
    "\n",
    "f.close()\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training/validation/testing set\n",
    "\n",
    "Now that we have our dataset, we need to create a training set, a validation set and a testing set. We have decided that our AI will just read Word or PDF file only (possibly that in the future that we had other format). It will be easier to do a supervised learning. So, we'll just select a lot of file and labelised them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\apira\\anaconda3\\lib\\site-packages (2.11.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from PyPDF2) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 13/304 [00:14<05:08,  1.06s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  5%|▍         | 15/304 [00:14<02:55,  1.65it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  5%|▌         | 16/304 [00:14<02:21,  2.03it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  6%|▌         | 18/304 [00:14<01:33,  3.06it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  6%|▋         | 19/304 [00:15<01:25,  3.35it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  7%|▋         | 21/304 [00:15<01:38,  2.88it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  7%|▋         | 22/304 [00:16<01:38,  2.85it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  8%|▊         | 23/304 [00:16<01:51,  2.52it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  8%|▊         | 25/304 [00:17<01:25,  3.28it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  9%|▊         | 26/304 [00:18<02:06,  2.20it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "  9%|▉         | 28/304 [00:18<01:30,  3.06it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 10%|▉         | 30/304 [00:18<01:08,  3.98it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 11%|█         | 32/304 [00:18<00:57,  4.77it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 11%|█         | 33/304 [00:19<01:13,  3.68it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 11%|█         | 34/304 [00:19<01:07,  4.03it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 12%|█▏        | 35/304 [00:19<01:01,  4.37it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 12%|█▎        | 38/304 [00:20<00:38,  6.87it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 13%|█▎        | 40/304 [00:20<00:43,  6.04it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 13%|█▎        | 41/304 [00:20<00:56,  4.62it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 15%|█▌        | 47/304 [00:23<01:18,  3.27it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 16%|█▋        | 50/304 [00:23<00:52,  4.86it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 17%|█▋        | 51/304 [00:24<00:52,  4.85it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 17%|█▋        | 52/304 [00:24<00:48,  5.15it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 18%|█▊        | 55/304 [00:26<02:30,  1.65it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 19%|█▉        | 57/304 [00:26<01:35,  2.58it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 19%|█▉        | 58/304 [00:26<01:16,  3.21it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 19%|█▉        | 59/304 [00:27<01:05,  3.72it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 20%|██        | 61/304 [00:27<00:49,  4.87it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 20%|██        | 62/304 [00:27<00:47,  5.11it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 21%|██        | 63/304 [00:27<00:52,  4.63it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 21%|██▏       | 65/304 [00:28<00:38,  6.21it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 22%|██▏       | 66/304 [00:28<00:36,  6.45it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 22%|██▏       | 67/304 [00:28<00:50,  4.65it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 22%|██▏       | 68/304 [00:28<00:57,  4.11it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 23%|██▎       | 69/304 [00:29<00:53,  4.39it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 23%|██▎       | 70/304 [00:29<01:07,  3.48it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 23%|██▎       | 71/304 [00:30<01:45,  2.21it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 24%|██▍       | 74/304 [00:31<01:34,  2.45it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 25%|██▌       | 76/304 [00:31<01:01,  3.73it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 26%|██▌       | 79/304 [00:32<00:43,  5.19it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 26%|██▋       | 80/304 [00:32<00:43,  5.17it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 27%|██▋       | 81/304 [00:32<00:43,  5.08it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 27%|██▋       | 82/304 [00:32<00:41,  5.37it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 28%|██▊       | 84/304 [00:32<00:36,  5.96it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 28%|██▊       | 85/304 [00:33<00:34,  6.28it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 29%|██▊       | 87/304 [00:33<00:32,  6.65it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 29%|██▉       | 88/304 [00:33<00:33,  6.35it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 31%|███       | 93/304 [00:34<00:39,  5.37it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 32%|███▏      | 96/304 [00:35<00:38,  5.47it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 33%|███▎      | 99/304 [00:35<00:50,  4.09it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 34%|███▎      | 102/304 [00:40<03:31,  1.05s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 34%|███▍      | 104/304 [00:54<10:45,  3.23s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 35%|███▍      | 105/304 [00:55<08:53,  2.68s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 36%|███▌      | 109/304 [00:56<03:53,  1.20s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 37%|███▋      | 111/304 [00:57<02:42,  1.19it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 38%|███▊      | 115/304 [00:57<01:13,  2.57it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 38%|███▊      | 117/304 [00:58<01:10,  2.64it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 39%|███▉      | 118/304 [00:58<00:59,  3.15it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 39%|███▉      | 119/304 [01:00<01:41,  1.83it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 40%|███▉      | 121/304 [01:01<01:38,  1.86it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 40%|████      | 122/304 [01:01<01:24,  2.16it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 40%|████      | 123/304 [01:01<01:09,  2.62it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 41%|████▏     | 126/304 [01:02<01:06,  2.66it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 43%|████▎     | 130/304 [01:03<01:06,  2.60it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 43%|████▎     | 131/304 [01:04<01:02,  2.78it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 44%|████▍     | 134/304 [01:04<00:42,  3.97it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 44%|████▍     | 135/304 [01:05<01:01,  2.76it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 45%|████▍     | 136/304 [01:05<01:01,  2.71it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 45%|████▌     | 137/304 [01:06<00:55,  3.00it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 46%|████▌     | 139/304 [01:06<00:43,  3.82it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 46%|████▌     | 140/304 [01:06<00:46,  3.52it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 47%|████▋     | 142/304 [01:06<00:35,  4.52it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 47%|████▋     | 143/304 [01:07<00:36,  4.41it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 47%|████▋     | 144/304 [01:07<00:49,  3.20it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 48%|████▊     | 146/304 [01:08<00:47,  3.34it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 48%|████▊     | 147/304 [01:08<00:52,  2.98it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 49%|████▉     | 149/304 [01:10<01:07,  2.28it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 50%|████▉     | 151/304 [01:10<00:46,  3.30it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 50%|█████     | 153/304 [01:10<00:37,  4.07it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 51%|█████     | 154/304 [01:10<00:38,  3.90it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 51%|█████▏    | 156/304 [01:11<00:46,  3.20it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 52%|█████▏    | 158/304 [01:12<00:40,  3.58it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 53%|█████▎    | 162/304 [01:12<00:29,  4.87it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 54%|█████▍    | 164/304 [01:12<00:24,  5.77it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 54%|█████▍    | 165/304 [01:13<00:24,  5.62it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 55%|█████▍    | 166/304 [01:13<00:23,  5.90it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 57%|█████▋    | 172/304 [01:15<00:45,  2.92it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 57%|█████▋    | 173/304 [01:16<00:44,  2.96it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 58%|█████▊    | 177/304 [01:18<00:55,  2.28it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 59%|█████▉    | 179/304 [01:18<00:37,  3.31it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 60%|█████▉    | 182/304 [01:18<00:25,  4.78it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 60%|██████    | 183/304 [01:19<00:32,  3.75it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 61%|██████    | 184/304 [01:19<00:30,  3.97it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 61%|██████    | 185/304 [01:20<01:09,  1.71it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 61%|██████    | 186/304 [01:21<00:53,  2.19it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 62%|██████▏   | 188/304 [01:21<00:35,  3.23it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 62%|██████▎   | 190/304 [01:21<00:30,  3.76it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 63%|██████▎   | 193/304 [01:22<00:42,  2.64it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 64%|██████▍   | 194/304 [01:23<00:34,  3.16it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 64%|██████▍   | 196/304 [01:23<00:30,  3.55it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 65%|██████▍   | 197/304 [01:23<00:29,  3.66it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 65%|██████▌   | 198/304 [01:23<00:25,  4.09it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 65%|██████▌   | 199/304 [01:24<00:24,  4.32it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 66%|██████▌   | 200/304 [01:24<00:24,  4.24it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 66%|██████▋   | 202/304 [01:24<00:19,  5.24it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 68%|██████▊   | 208/304 [01:27<00:46,  2.08it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 69%|██████▉   | 210/304 [01:27<00:33,  2.80it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 70%|██████▉   | 212/304 [01:28<00:32,  2.87it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 70%|███████   | 213/304 [01:28<00:29,  3.11it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 70%|███████   | 214/304 [01:29<00:36,  2.46it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 71%|███████▏  | 217/304 [01:29<00:18,  4.69it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 72%|███████▏  | 219/304 [01:29<00:16,  5.06it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 73%|███████▎  | 221/304 [01:34<01:18,  1.05it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 73%|███████▎  | 222/304 [01:34<01:04,  1.26it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 74%|███████▍  | 225/304 [01:35<00:41,  1.93it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 75%|███████▍  | 227/304 [01:36<00:32,  2.38it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 75%|███████▌  | 228/304 [01:36<00:27,  2.80it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 76%|███████▌  | 231/304 [01:37<00:23,  3.11it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 76%|███████▋  | 232/304 [01:37<00:19,  3.74it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 77%|███████▋  | 234/304 [01:38<00:21,  3.29it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 78%|███████▊  | 237/304 [01:40<00:35,  1.91it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 78%|███████▊  | 238/304 [01:40<00:32,  2.01it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 79%|███████▉  | 241/304 [01:41<00:26,  2.39it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 80%|███████▉  | 242/304 [01:41<00:22,  2.80it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 81%|████████  | 245/304 [01:50<01:39,  1.69s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 81%|████████  | 246/304 [01:51<01:18,  1.35s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 82%|████████▏ | 248/304 [01:51<00:48,  1.16it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 82%|████████▏ | 250/304 [01:52<00:33,  1.59it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 83%|████████▎ | 252/304 [01:52<00:27,  1.89it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 83%|████████▎ | 253/304 [01:53<00:22,  2.22it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 84%|████████▎ | 254/304 [01:53<00:20,  2.47it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 85%|████████▍ | 257/304 [01:53<00:13,  3.47it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 85%|████████▍ | 258/304 [01:54<00:11,  3.95it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 85%|████████▌ | 259/304 [01:54<00:10,  4.17it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 86%|████████▌ | 262/304 [01:55<00:16,  2.48it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 87%|████████▋ | 264/304 [01:56<00:12,  3.26it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 88%|████████▊ | 268/304 [01:57<00:08,  4.45it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 89%|████████▉ | 272/304 [02:31<02:22,  4.44s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 90%|████████▉ | 273/304 [02:32<01:55,  3.72s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 90%|█████████ | 274/304 [02:32<01:30,  3.03s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 91%|█████████ | 276/304 [02:33<00:52,  1.88s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 92%|█████████▏| 279/304 [02:33<00:26,  1.06s/it]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 92%|█████████▏| 281/304 [02:34<00:19,  1.16it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 93%|█████████▎| 282/304 [02:34<00:16,  1.35it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 93%|█████████▎| 283/304 [02:34<00:13,  1.61it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 94%|█████████▍| 286/304 [02:34<00:06,  2.78it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 95%|█████████▌| 290/304 [02:35<00:03,  4.48it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 96%|█████████▌| 292/304 [02:35<00:02,  4.46it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      " 96%|█████████▋| 293/304 [02:36<00:03,  3.30it/s]Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "100%|██████████| 304/304 [02:46<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "key = ['biology', 'compsci', 'physics', 'chemistry', 'philosophy']\n",
    "index = dict()\n",
    "for ind in range(0,len(key)):\n",
    "    index[key[ind]] = ind\n",
    "\n",
    "# Path towards the folder where there are all files\n",
    "folder_path = os.path.abspath(os.getcwd()) + '\\FileForTraining'\n",
    "\n",
    "# For each file, we will count\n",
    "scores = list()\n",
    "data_filename_topics = pd.read_csv('Dataset_fileName-Topics.csv')\n",
    "for filename, _ in tqdm(data_filename_topics.values):\n",
    "    file = os.path.join(folder_path, filename)\n",
    "    if(os.path.isfile(file)):\n",
    "        text = None\n",
    "        extension = os.path.splitext(file)[1]\n",
    "        if extension == \".pdf\":  # If the file is a pdf file\n",
    "            with open(file, 'rb') as pdfFileObj:\n",
    "                pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "                text = re.sub(r'[^\\w\\s]', ' ', pdfReader.getPage(0).extractText())\n",
    "                for pageNumber in range(pdfReader.numPages):\n",
    "                    pageObj = pdfReader.getPage(pageNumber)\n",
    "                    pageText = re.sub(r'[^\\w\\s]', ' ', pageObj.extractText())\n",
    "                    text = ' '.join([text, pageText])\n",
    "\n",
    "                text = text.split(' ')\n",
    "        elif extension == \".doc\" or extension == \".docx\":  # If the file is a docx or doc\n",
    "            # text = textfromword(file)\n",
    "            pass\n",
    "\n",
    "        # If the file is a pdf or a word, we can compute his score\n",
    "        if text != None:\n",
    "            score = np.zeros(len(key))\n",
    "            for word in text:\n",
    "                w = word.lower()\n",
    "                for subject in dataset:\n",
    "                    if(w in dataset[subject]):\n",
    "                        score[index[subject]] += 1\n",
    "            scores.append(score)\n",
    "    else:\n",
    "        print(\"The file\", file, \"is not supported.\")\n",
    "\n",
    "# We decide to put all those information in dataframe\n",
    "df_x = pd.DataFrame(np.array(scores), columns = key)\n",
    "df_y = data_filename_topics['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     biology  compsci  physics  chemistry  philosophy\n",
      "0        0.0     21.0      0.0        4.0         1.0\n",
      "1        0.0     52.0      1.0        2.0         2.0\n",
      "2        1.0     98.0      1.0        3.0         6.0\n",
      "3        2.0    144.0      7.0        2.0        11.0\n",
      "4        0.0    143.0      3.0        4.0        10.0\n",
      "..       ...      ...      ...        ...         ...\n",
      "299    161.0     52.0     23.0       51.0        20.0\n",
      "300    617.0    256.0    209.0      115.0       150.0\n",
      "301     46.0    688.0   1772.0      672.0      1130.0\n",
      "302      4.0     10.0      4.0        3.0        11.0\n",
      "303      6.0    105.0    289.0       65.0       280.0\n",
      "\n",
      "[304 rows x 5 columns]\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "299    0\n",
      "300    0\n",
      "301    2\n",
      "302    4\n",
      "303    2\n",
      "Name: topic, Length: 304, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_x)\n",
    "print(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biology: 48\n",
      "compsci: 83\n",
      "physics: 96\n",
      "chemistry: 46\n",
      "philosophy: 31\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(key)):\n",
    "    print(f\"{key[i]}: {len(df_y[df_y == i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataframe, we have to split it into 3 sets : training, validation, testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.htmlNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch==1.12.0+cpu in c:\\users\\apira\\anaconda3\\lib\\site-packages (1.12.0+cpu)\n",
      "Requirement already satisfied: torchvision==0.13.0+cpu in c:\\users\\apira\\anaconda3\\lib\\site-packages (0.13.0+cpu)\n",
      "Requirement already satisfied: torchaudio==0.12.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torch==1.12.0+cpu) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\apira\\anaconda3\\lib\\site-packages (from torchvision==0.13.0+cpu) (1.21.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apira\\anaconda3\\lib\\site-packages (from requests->torchvision==0.13.0+cpu) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.12.0+cpu torchvision==0.13.0+cpu torchaudio==0.12.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch as t\n",
    "\n",
    "# Split the data into 70% for training, 15% for validation and 15% for testing\n",
    "train_x, rest_x, train_y, rest_y = train_test_split(df_x.values, df_y.values, train_size=0.7)\n",
    "val_x, test_x, val_y, test_y = train_test_split(rest_x, rest_y, train_size=0.5)\n",
    "\n",
    "# Transformation and normalization\n",
    "train_x = t.tensor(train_x, dtype = t.float32)\n",
    "val_x = t.tensor(val_x, dtype = t.float32)\n",
    "test_x = t.tensor(test_x, dtype = t.float32)\n",
    "\n",
    "train_y = t.tensor(train_y, dtype= int)\n",
    "val_y = t.tensor(val_y, dtype= int)\n",
    "test_y = t.tensor(test_y, dtype= int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    # Inputs to hidden layer linear transformation\n",
    "    self.input = nn.Linear(D_in, H)\n",
    "    self.hidden = nn.Linear(H, H)\n",
    "    self.output = nn.Linear(H, D_out)\n",
    "\n",
    "    self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.input(x))\n",
    "    x = F.relu(self.hidden(x))\n",
    "    y_pred = self.output(x)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_x, train_y, val_x, val_y, num_epochs = 10, batch_size = 64, show_info = False):\n",
    "  # Set model to train mode\n",
    "  model.train()\n",
    "\n",
    "  # Training loop\n",
    "  for epoch in range(0,num_epochs):\n",
    "    perm = t.randperm(len(train_y))\n",
    "    sum_loss = 0.\n",
    "\n",
    "    for i in range(0, len(train_y), batch_size):\n",
    "      x1 = Variable(train_x[perm[i:i + batch_size]], requires_grad=False)\n",
    "      y1 = Variable(train_y[perm[i:i + batch_size]], requires_grad=False)\n",
    "\n",
    "      # Reset gradient\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Forward\n",
    "      fx = model(x1)\n",
    "      loss = criterion(fx, y1)\n",
    "      \n",
    "      # Backward\n",
    "      loss.backward()\n",
    "      \n",
    "      # Update parameters\n",
    "      optimizer.step()\n",
    "      \n",
    "      sum_loss += loss.item()\n",
    "\n",
    "    val_loss = validation_model(model, criterion, val_x, val_y, batch_size)\n",
    "    if(show_info):\n",
    "      print(f\"Epoch: {epoch+1}\\tTraining Loss: {sum_loss}\\tValidation Loss: {val_loss}\")\n",
    "\n",
    "def validation_model(model, criterion, val_x, val_y, batch_size):\n",
    "  valid_loss = 0\n",
    "  perm = t.randperm(len(val_y))\n",
    "\n",
    "  # Set to validation mode\n",
    "  model.eval()\n",
    "  \n",
    "  for i in range(0, len(val_y), batch_size):\n",
    "      x1 = Variable(val_x[perm[i:i + batch_size]], requires_grad=False)\n",
    "      y1 = Variable(val_y[perm[i:i + batch_size]], requires_grad=False)\n",
    "      \n",
    "      # Forward\n",
    "      fx = model(x1)\n",
    "      loss = criterion(fx, y1)\n",
    "      \n",
    "      valid_loss += loss.item()\n",
    "\n",
    "  return valid_loss\n",
    "\n",
    "def evaluate_model(model, test_x, test_y):\n",
    "  model.eval()\n",
    "  y_pred = model(test_x)\n",
    "\n",
    "  y_pred = t.max(y_pred,1).indices\n",
    "  accuracy =  t.mean(t.Tensor([i == j for i, j in zip(y_pred, test_y)]))\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTraining Loss: 80.41137282550335\tValidation Loss: 22.816313683986664\n",
      "Epoch: 2\tTraining Loss: 68.61460008472204\tValidation Loss: 4.923495844006538\n",
      "Epoch: 3\tTraining Loss: 50.0225683003664\tValidation Loss: 10.803473770618439\n",
      "Epoch: 4\tTraining Loss: 43.37522664666176\tValidation Loss: 4.83735679090023\n",
      "Epoch: 5\tTraining Loss: 47.27895198762417\tValidation Loss: 8.270422205328941\n",
      "Epoch: 6\tTraining Loss: 55.34852270781994\tValidation Loss: 7.3193947076797485\n",
      "Epoch: 7\tTraining Loss: 25.690837129950523\tValidation Loss: 7.590088665485382\n",
      "Epoch: 8\tTraining Loss: 41.590294390916824\tValidation Loss: 4.740236647427082\n",
      "Epoch: 9\tTraining Loss: 31.90928490459919\tValidation Loss: 5.866961777210236\n",
      "Epoch: 10\tTraining Loss: 26.048245646059513\tValidation Loss: 8.449291855096817\n",
      "Epoch: 11\tTraining Loss: 33.00748674571514\tValidation Loss: 9.816524595022202\n",
      "Epoch: 12\tTraining Loss: 28.086455792188644\tValidation Loss: 5.482224375009537\n",
      "Epoch: 13\tTraining Loss: 24.363523855805397\tValidation Loss: 5.233165502548218\n",
      "Epoch: 14\tTraining Loss: 21.381991785019636\tValidation Loss: 3.900552809238434\n",
      "Epoch: 15\tTraining Loss: 27.68434353172779\tValidation Loss: 7.978900834918022\n",
      "Epoch: 16\tTraining Loss: 21.914476960897446\tValidation Loss: 10.128711313009262\n",
      "Epoch: 17\tTraining Loss: 18.41737823188305\tValidation Loss: 4.498013228178024\n",
      "Epoch: 18\tTraining Loss: 14.584146168082952\tValidation Loss: 4.710947036743164\n",
      "Epoch: 19\tTraining Loss: 14.055300168693066\tValidation Loss: 4.9172483086586\n",
      "Epoch: 20\tTraining Loss: 14.192664384841919\tValidation Loss: 4.227068595588207\n",
      "Epoch: 21\tTraining Loss: 13.383521482348442\tValidation Loss: 5.085273623466492\n",
      "Epoch: 22\tTraining Loss: 42.11465089768171\tValidation Loss: 7.8547782599925995\n",
      "Epoch: 23\tTraining Loss: 50.22525283694267\tValidation Loss: 8.284833371639252\n",
      "Epoch: 24\tTraining Loss: 23.805813923478127\tValidation Loss: 5.6049032509326935\n",
      "Epoch: 25\tTraining Loss: 15.784268632531166\tValidation Loss: 6.767098784446716\n",
      "Epoch: 26\tTraining Loss: 14.982213944196701\tValidation Loss: 6.902639001607895\n",
      "Epoch: 27\tTraining Loss: 14.944667175412178\tValidation Loss: 6.373341903090477\n",
      "Epoch: 28\tTraining Loss: 14.894165277481079\tValidation Loss: 4.175027549266815\n",
      "Epoch: 29\tTraining Loss: 13.165679428726435\tValidation Loss: 5.174781680107117\n",
      "Epoch: 30\tTraining Loss: 15.35431606322527\tValidation Loss: 4.241057708859444\n",
      "Epoch: 31\tTraining Loss: 37.857113452628255\tValidation Loss: 6.0610960721969604\n",
      "Epoch: 32\tTraining Loss: 23.709571793675423\tValidation Loss: 4.3268313109874725\n",
      "Epoch: 33\tTraining Loss: 19.114054694771767\tValidation Loss: 4.6942554116249084\n",
      "Epoch: 34\tTraining Loss: 14.50015452504158\tValidation Loss: 4.4068964421749115\n",
      "Epoch: 35\tTraining Loss: 13.285644620656967\tValidation Loss: 7.345458284020424\n",
      "Epoch: 36\tTraining Loss: 13.538822963833809\tValidation Loss: 4.740591984242201\n",
      "Epoch: 37\tTraining Loss: 13.18793299794197\tValidation Loss: 4.772324327379465\n",
      "Epoch: 38\tTraining Loss: 22.09396844357252\tValidation Loss: 4.951180383563042\n",
      "Epoch: 39\tTraining Loss: 16.908769890666008\tValidation Loss: 4.638547018170357\n",
      "Epoch: 40\tTraining Loss: 13.236591033637524\tValidation Loss: 7.445364147424698\n",
      "Epoch: 41\tTraining Loss: 12.77319298684597\tValidation Loss: 5.173616588115692\n",
      "Epoch: 42\tTraining Loss: 12.089645832777023\tValidation Loss: 4.797069475054741\n",
      "Epoch: 43\tTraining Loss: 11.0360196698457\tValidation Loss: 6.149491131305695\n",
      "Epoch: 44\tTraining Loss: 11.201430980116129\tValidation Loss: 4.782229475677013\n",
      "Epoch: 45\tTraining Loss: 13.610557832755148\tValidation Loss: 5.186823174357414\n",
      "Epoch: 46\tTraining Loss: 10.604679826647043\tValidation Loss: 5.314170405268669\n",
      "Epoch: 47\tTraining Loss: 9.969994205981493\tValidation Loss: 5.003996409475803\n",
      "Epoch: 48\tTraining Loss: 10.054869256913662\tValidation Loss: 5.404229491949081\n",
      "Epoch: 49\tTraining Loss: 9.525558561086655\tValidation Loss: 5.5246594697237015\n",
      "Epoch: 50\tTraining Loss: 9.90739855915308\tValidation Loss: 5.67921756207943\n",
      "Epoch: 51\tTraining Loss: 9.564198289066553\tValidation Loss: 5.163416922092438\n",
      "Epoch: 52\tTraining Loss: 9.359044939279556\tValidation Loss: 5.654230177402496\n",
      "Epoch: 53\tTraining Loss: 8.104915272444487\tValidation Loss: 6.375513657927513\n",
      "Epoch: 54\tTraining Loss: 13.626850418746471\tValidation Loss: 5.308650583028793\n",
      "Epoch: 55\tTraining Loss: 8.429694637656212\tValidation Loss: 5.384828358888626\n",
      "Epoch: 56\tTraining Loss: 8.41609402000904\tValidation Loss: 5.620457828044891\n",
      "Epoch: 57\tTraining Loss: 8.160193905234337\tValidation Loss: 5.703892529010773\n",
      "Epoch: 58\tTraining Loss: 8.08048265427351\tValidation Loss: 5.65341728925705\n",
      "Epoch: 59\tTraining Loss: 10.338619865477085\tValidation Loss: 5.675982251763344\n",
      "Epoch: 60\tTraining Loss: 15.057262234389782\tValidation Loss: 5.2628737054765224\n",
      "Epoch: 61\tTraining Loss: 10.056971460580826\tValidation Loss: 6.27564537525177\n",
      "Epoch: 62\tTraining Loss: 7.934411961585283\tValidation Loss: 5.544836420565844\n",
      "Epoch: 63\tTraining Loss: 8.412456702440977\tValidation Loss: 5.610055446624756\n",
      "Epoch: 64\tTraining Loss: 8.47675634920597\tValidation Loss: 5.800250709056854\n",
      "Epoch: 65\tTraining Loss: 8.033852495253086\tValidation Loss: 5.493041526526213\n",
      "Epoch: 66\tTraining Loss: 7.966459862887859\tValidation Loss: 6.138408988714218\n",
      "Epoch: 67\tTraining Loss: 7.860448773950338\tValidation Loss: 6.021807253360748\n",
      "Epoch: 68\tTraining Loss: 8.128274589776993\tValidation Loss: 5.960389047861099\n",
      "Epoch: 69\tTraining Loss: 8.062136882450432\tValidation Loss: 6.375393062829971\n",
      "Epoch: 70\tTraining Loss: 7.223583827726543\tValidation Loss: 6.131495451554656\n",
      "Epoch: 71\tTraining Loss: 7.8100031688809395\tValidation Loss: 7.095787942409515\n",
      "Epoch: 72\tTraining Loss: 18.32859243825078\tValidation Loss: 8.255820289254189\n",
      "Epoch: 73\tTraining Loss: 10.136296931654215\tValidation Loss: 6.180550649762154\n",
      "Epoch: 74\tTraining Loss: 8.094340473413467\tValidation Loss: 6.830977618694305\n",
      "Epoch: 75\tTraining Loss: 7.483357220888138\tValidation Loss: 6.5225100964307785\n",
      "Epoch: 76\tTraining Loss: 6.997843228280544\tValidation Loss: 6.543734835460782\n",
      "Epoch: 77\tTraining Loss: 8.663000483065844\tValidation Loss: 7.577042274177074\n",
      "Epoch: 78\tTraining Loss: 7.696789160370827\tValidation Loss: 6.993208274245262\n",
      "Epoch: 79\tTraining Loss: 7.040644911117852\tValidation Loss: 7.218544036149979\n",
      "Epoch: 80\tTraining Loss: 8.886734565137886\tValidation Loss: 7.1768480613827705\n",
      "Epoch: 81\tTraining Loss: 7.3908583141019335\tValidation Loss: 6.324787966907024\n",
      "Epoch: 82\tTraining Loss: 6.643308651633561\tValidation Loss: 7.6223650723695755\n",
      "Epoch: 83\tTraining Loss: 6.333688918501139\tValidation Loss: 6.525349378585815\n",
      "Epoch: 84\tTraining Loss: 7.117363076657057\tValidation Loss: 7.49311188608408\n",
      "Epoch: 85\tTraining Loss: 6.050912883132696\tValidation Loss: 6.97283098846674\n",
      "Epoch: 86\tTraining Loss: 6.338955081999302\tValidation Loss: 7.559290111064911\n",
      "Epoch: 87\tTraining Loss: 6.383323104120791\tValidation Loss: 7.738882631063461\n",
      "Epoch: 88\tTraining Loss: 5.698435909114778\tValidation Loss: 7.479668736457825\n",
      "Epoch: 89\tTraining Loss: 6.380069329403341\tValidation Loss: 7.368668934330344\n",
      "Epoch: 90\tTraining Loss: 5.607111277058721\tValidation Loss: 7.535012021660805\n",
      "Epoch: 91\tTraining Loss: 5.355500320903957\tValidation Loss: 7.728306546807289\n",
      "Epoch: 92\tTraining Loss: 8.538005167618394\tValidation Loss: 7.4672536216676235\n",
      "Epoch: 93\tTraining Loss: 8.524644117802382\tValidation Loss: 6.787007417529821\n",
      "Epoch: 94\tTraining Loss: 33.92021568492055\tValidation Loss: 9.166025303304195\n",
      "Epoch: 95\tTraining Loss: 19.133386131376028\tValidation Loss: 12.22283387184143\n",
      "Epoch: 96\tTraining Loss: 22.568654358386993\tValidation Loss: 8.196578927338123\n",
      "Epoch: 97\tTraining Loss: 10.795240722596645\tValidation Loss: 8.286681354045868\n",
      "Epoch: 98\tTraining Loss: 6.738391857594252\tValidation Loss: 6.571763396263123\n",
      "Epoch: 99\tTraining Loss: 7.19761503720656\tValidation Loss: 7.270640939474106\n",
      "Epoch: 100\tTraining Loss: 8.212318408768624\tValidation Loss: 6.930655479431152\n",
      "Epoch: 101\tTraining Loss: 6.667065769433975\tValidation Loss: 7.115396708250046\n",
      "Epoch: 102\tTraining Loss: 5.880651358515024\tValidation Loss: 7.656026314944029\n",
      "Epoch: 103\tTraining Loss: 5.7777493465691805\tValidation Loss: 7.487658962607384\n",
      "Epoch: 104\tTraining Loss: 9.150399273261428\tValidation Loss: 7.3674863036721945\n",
      "Epoch: 105\tTraining Loss: 5.631083358079195\tValidation Loss: 7.348535001277924\n",
      "Epoch: 106\tTraining Loss: 6.24092353740707\tValidation Loss: 7.312236588448286\n",
      "Epoch: 107\tTraining Loss: 5.60349789634347\tValidation Loss: 7.51331639289856\n",
      "Epoch: 108\tTraining Loss: 5.544262012466788\tValidation Loss: 7.982840146869421\n",
      "Epoch: 109\tTraining Loss: 5.4455288629978895\tValidation Loss: 7.783193212002516\n",
      "Epoch: 110\tTraining Loss: 5.804890004917979\tValidation Loss: 7.649731852114201\n",
      "Epoch: 111\tTraining Loss: 5.013977237045765\tValidation Loss: 8.093536600470543\n",
      "Epoch: 112\tTraining Loss: 4.7627598736435175\tValidation Loss: 7.79988893866539\n",
      "Epoch: 113\tTraining Loss: 4.807353738695383\tValidation Loss: 8.036808729171753\n",
      "Epoch: 114\tTraining Loss: 4.817322216928005\tValidation Loss: 8.333291575312614\n",
      "Epoch: 115\tTraining Loss: 4.369927033782005\tValidation Loss: 8.075898591428995\n",
      "Epoch: 116\tTraining Loss: 5.806056005880237\tValidation Loss: 8.257498323917389\n",
      "Epoch: 117\tTraining Loss: 4.8740621246397495\tValidation Loss: 8.183459848165512\n",
      "Epoch: 118\tTraining Loss: 4.737745560705662\tValidation Loss: 8.07983625959605\n",
      "Epoch: 119\tTraining Loss: 4.399579355493188\tValidation Loss: 8.771477729082108\n",
      "Epoch: 120\tTraining Loss: 4.333119275746867\tValidation Loss: 8.285300329327583\n",
      "Epoch: 121\tTraining Loss: 5.114648870192468\tValidation Loss: 9.338517844676971\n",
      "Epoch: 122\tTraining Loss: 5.018872020766139\tValidation Loss: 9.662120819091797\n",
      "Epoch: 123\tTraining Loss: 4.2179602868855\tValidation Loss: 9.24984185025096\n",
      "Epoch: 124\tTraining Loss: 4.117209862917662\tValidation Loss: 8.768691390752792\n",
      "Epoch: 125\tTraining Loss: 4.15953160263598\tValidation Loss: 9.064276441931725\n",
      "Epoch: 126\tTraining Loss: 4.799242837820202\tValidation Loss: 8.7321034851484\n",
      "Epoch: 127\tTraining Loss: 3.988130956888199\tValidation Loss: 8.847890742123127\n",
      "Epoch: 128\tTraining Loss: 3.7714334782212973\tValidation Loss: 9.061290733516216\n",
      "Epoch: 129\tTraining Loss: 4.530843026004732\tValidation Loss: 9.6631880402565\n",
      "Epoch: 130\tTraining Loss: 6.212128099054098\tValidation Loss: 9.779331404715776\n",
      "Epoch: 131\tTraining Loss: 7.74686192907393\tValidation Loss: 9.736713290214539\n",
      "Epoch: 132\tTraining Loss: 4.602711300365627\tValidation Loss: 9.60024607181549\n",
      "Epoch: 133\tTraining Loss: 4.491839146474376\tValidation Loss: 9.02277684211731\n",
      "Epoch: 134\tTraining Loss: 4.04611742682755\tValidation Loss: 9.24579918384552\n",
      "Epoch: 135\tTraining Loss: 4.871612130198628\tValidation Loss: 9.729353923350573\n",
      "Epoch: 136\tTraining Loss: 3.8330337768420577\tValidation Loss: 9.758529305458069\n",
      "Epoch: 137\tTraining Loss: 3.5254680563230067\tValidation Loss: 9.877427399158478\n",
      "Epoch: 138\tTraining Loss: 3.5476360134780407\tValidation Loss: 9.273282825946808\n",
      "Epoch: 139\tTraining Loss: 3.336755166761577\tValidation Loss: 10.34292346984148\n",
      "Epoch: 140\tTraining Loss: 3.7985034096054733\tValidation Loss: 9.776824398897588\n",
      "Epoch: 141\tTraining Loss: 3.1069953283295035\tValidation Loss: 9.849810764193535\n",
      "Epoch: 142\tTraining Loss: 3.5262962174601853\tValidation Loss: 10.139310836791992\n",
      "Epoch: 143\tTraining Loss: 3.485409092158079\tValidation Loss: 10.597192598506808\n",
      "Epoch: 144\tTraining Loss: 6.170356215094216\tValidation Loss: 10.359085008502007\n",
      "Epoch: 145\tTraining Loss: 3.688737669494003\tValidation Loss: 9.718349784612656\n",
      "Epoch: 146\tTraining Loss: 3.2581153446808457\tValidation Loss: 10.137211680412292\n",
      "Epoch: 147\tTraining Loss: 3.474953101016581\tValidation Loss: 10.91369317099452\n",
      "Epoch: 148\tTraining Loss: 2.9317717207595706\tValidation Loss: 9.805275186896324\n",
      "Epoch: 149\tTraining Loss: 3.721860612044111\tValidation Loss: 10.704935848712921\n",
      "Epoch: 150\tTraining Loss: 25.664520697668195\tValidation Loss: 11.096435658633709\n",
      "Epoch: 151\tTraining Loss: 49.554604666584055\tValidation Loss: 23.220131188631058\n",
      "Epoch: 152\tTraining Loss: 72.76804189104587\tValidation Loss: 42.27344414964318\n",
      "Epoch: 153\tTraining Loss: 44.33923265710473\tValidation Loss: 11.374758414924145\n",
      "Epoch: 154\tTraining Loss: 14.968038585036993\tValidation Loss: 9.32469963375479\n",
      "Epoch: 155\tTraining Loss: 7.840061586350203\tValidation Loss: 9.787908390164375\n",
      "Epoch: 156\tTraining Loss: 5.944968406111002\tValidation Loss: 9.058894082685583\n",
      "Epoch: 157\tTraining Loss: 5.424067858606577\tValidation Loss: 8.984153728932142\n",
      "Epoch: 158\tTraining Loss: 6.394506591372192\tValidation Loss: 9.109657257795334\n",
      "Epoch: 159\tTraining Loss: 4.980099705979228\tValidation Loss: 11.36451455578208\n",
      "Epoch: 160\tTraining Loss: 4.683296477422118\tValidation Loss: 9.390232771635056\n",
      "Epoch: 161\tTraining Loss: 4.610882761422545\tValidation Loss: 10.181091159582138\n",
      "Epoch: 162\tTraining Loss: 4.000042196363211\tValidation Loss: 9.60813210066408\n",
      "Epoch: 163\tTraining Loss: 4.274812104180455\tValidation Loss: 9.28532880358398\n",
      "Epoch: 164\tTraining Loss: 3.6267893919721246\tValidation Loss: 10.45012491941452\n",
      "Epoch: 165\tTraining Loss: 3.5984814632683992\tValidation Loss: 9.827320277690887\n",
      "Epoch: 166\tTraining Loss: 3.558526110369712\tValidation Loss: 10.363211907446384\n",
      "Epoch: 167\tTraining Loss: 3.6959596753586084\tValidation Loss: 9.616352058947086\n",
      "Epoch: 168\tTraining Loss: 3.3395660426467657\tValidation Loss: 9.860372178256512\n",
      "Epoch: 169\tTraining Loss: 4.193174417130649\tValidation Loss: 9.595984926447272\n",
      "Epoch: 170\tTraining Loss: 3.5459263371303678\tValidation Loss: 9.567028850317001\n",
      "Epoch: 171\tTraining Loss: 4.159346962580457\tValidation Loss: 9.562955498695374\n",
      "Epoch: 172\tTraining Loss: 3.9537670135032386\tValidation Loss: 9.626049948856235\n",
      "Epoch: 173\tTraining Loss: 3.264871665276587\tValidation Loss: 9.924725316464901\n",
      "Epoch: 174\tTraining Loss: 3.3368955571204424\tValidation Loss: 10.054358191788197\n",
      "Epoch: 175\tTraining Loss: 3.499398835701868\tValidation Loss: 10.09058701992035\n",
      "Epoch: 176\tTraining Loss: 3.36253075953573\tValidation Loss: 11.724199429154396\n",
      "Epoch: 177\tTraining Loss: 3.5462168846279383\tValidation Loss: 10.706653375178576\n",
      "Epoch: 178\tTraining Loss: 3.077316233422607\tValidation Loss: 11.68154826760292\n",
      "Epoch: 179\tTraining Loss: 3.140058954246342\tValidation Loss: 10.36028902232647\n",
      "Epoch: 180\tTraining Loss: 3.3285501655191183\tValidation Loss: 11.446367491036654\n",
      "Epoch: 181\tTraining Loss: 3.597405263222754\tValidation Loss: 10.363289112159691\n",
      "Epoch: 182\tTraining Loss: 2.9514194192015566\tValidation Loss: 10.449421822326258\n",
      "Epoch: 183\tTraining Loss: 2.9235605988651514\tValidation Loss: 11.78259015828371\n",
      "Epoch: 184\tTraining Loss: 3.095617737621069\tValidation Loss: 10.467184705659747\n",
      "Epoch: 185\tTraining Loss: 3.160950838122517\tValidation Loss: 10.666432573838392\n",
      "Epoch: 186\tTraining Loss: 3.918102942989208\tValidation Loss: 11.612910125404596\n",
      "Epoch: 187\tTraining Loss: 3.8667115778662264\tValidation Loss: 12.28832944482565\n",
      "Epoch: 188\tTraining Loss: 17.86732371710241\tValidation Loss: 11.966281920671463\n",
      "Epoch: 189\tTraining Loss: 5.939128032885492\tValidation Loss: 11.10725086927414\n",
      "Epoch: 190\tTraining Loss: 8.372984000947326\tValidation Loss: 10.12023013830185\n",
      "Epoch: 191\tTraining Loss: 3.593609740724787\tValidation Loss: 9.959779858589172\n",
      "Epoch: 192\tTraining Loss: 4.066194505430758\tValidation Loss: 10.733883697539568\n",
      "Epoch: 193\tTraining Loss: 4.27825803111773\tValidation Loss: 9.977465726435184\n",
      "Epoch: 194\tTraining Loss: 4.484125322778709\tValidation Loss: 9.822560667991638\n",
      "Epoch: 195\tTraining Loss: 3.6809957632794976\tValidation Loss: 11.403676822781563\n",
      "Epoch: 196\tTraining Loss: 3.0293858502991498\tValidation Loss: 10.195841491222382\n",
      "Epoch: 197\tTraining Loss: 2.741737420670688\tValidation Loss: 10.176595462020487\n",
      "Epoch: 198\tTraining Loss: 2.812249163398519\tValidation Loss: 10.665017455816269\n",
      "Epoch: 199\tTraining Loss: 3.0397491892799735\tValidation Loss: 10.830457150936127\n",
      "Epoch: 200\tTraining Loss: 3.0578485908918083\tValidation Loss: 10.685237094759941\n",
      "Accuracy: 63.043479919433594 %\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 200\n",
    "batch_size = 8\n",
    "\n",
    "D_in, H, D_out = train_x.shape[1], 256, len(key)\n",
    "model = MLP(D_in, H, D_out)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_x, train_y,\n",
    "            val_x, val_y, epochs, batch_size, show_info = True)\n",
    "\n",
    "#Evaluate the model\n",
    "accuracy = evaluate_model(model, test_x, test_y)*100\n",
    "print(f'Accuracy: {accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import Dense\n",
    "\n",
    "model = Sequential(\n",
    "    Dense(D_in, activation = \"relu\"),\n",
    "    Dense(H, activation = \"relu\"),\n",
    "    Dense(D_out)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1862056fd60bcc08f314446936f371e203784f8dd6035e4c478d6b1676b9f55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
